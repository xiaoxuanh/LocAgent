{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/cloudfiles/code/Users/v-xhou/locagent/LocAgent/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.append('/home/czl/workspace/LocAgent')\n",
    "sys.path.append('/home/azureuser/cloudfiles/code/Users/v-xhou/locagent/LocAgent')\n",
    "\n",
    "from evaluation.eval_metric import evaluate_results\n",
    "level2key_dict = {\n",
    "    'file': 'found_files',\n",
    "    'module': 'found_modules',\n",
    "    'function': 'found_entities',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">file</th>\n",
       "      <th colspan=\"2\" halign=\"left\">module</th>\n",
       "      <th colspan=\"2\" halign=\"left\">function</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Acc@1</th>\n",
       "      <th>Acc@3</th>\n",
       "      <th>Acc@5</th>\n",
       "      <th>Acc@5</th>\n",
       "      <th>Acc@10</th>\n",
       "      <th>Acc@5</th>\n",
       "      <th>Acc@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.8139</td>\n",
       "      <td>0.8212</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.4453</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>0.3175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file                 module         function        \n",
       "    Acc@1   Acc@3   Acc@5  Acc@5  Acc@10    Acc@5  Acc@10\n",
       "0  0.6569  0.8139  0.8212  0.354  0.4453   0.2518  0.3175"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval with dataset\n",
    "locagent_loc_file = '../results/GPT4-1-mini0414/loc_outputs.jsonl'\n",
    "locagent_res = evaluate_results(locagent_loc_file,\n",
    "                        level2key_dict,\n",
    "                        dataset='czlll/SWE-bench_Lite', split='test', \n",
    "                        metrics=['acc'],\n",
    "                        # metrics=['ndcg'],\n",
    "                        )\n",
    "locagent_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">file</th>\n",
       "      <th colspan=\"2\" halign=\"left\">module</th>\n",
       "      <th colspan=\"2\" halign=\"left\">function</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Acc@1</th>\n",
       "      <th>Acc@3</th>\n",
       "      <th>Acc@5</th>\n",
       "      <th>Acc@5</th>\n",
       "      <th>Acc@10</th>\n",
       "      <th>Acc@5</th>\n",
       "      <th>Acc@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.8175</td>\n",
       "      <td>0.8248</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.4562</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.3577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file                 module         function        \n",
       "    Acc@1   Acc@3   Acc@5  Acc@5  Acc@10    Acc@5  Acc@10\n",
       "0  0.6788  0.8175  0.8248  0.354  0.4562    0.281  0.3577"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval with dataset\n",
    "locagent_loc_file = '../results/GPT4-1-mini0101/loc_outputs.jsonl'\n",
    "locagent_res = evaluate_results(locagent_loc_file,\n",
    "                        level2key_dict,\n",
    "                        dataset='czlll/SWE-bench_Lite', split='test', \n",
    "                        metrics=['acc'],\n",
    "                        # metrics=['ndcg'],\n",
    "                        )\n",
    "locagent_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">file</th>\n",
       "      <th colspan=\"2\" halign=\"left\">module</th>\n",
       "      <th colspan=\"2\" halign=\"left\">function</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Acc@1</th>\n",
       "      <th>Acc@3</th>\n",
       "      <th>Acc@5</th>\n",
       "      <th>Acc@5</th>\n",
       "      <th>Acc@10</th>\n",
       "      <th>Acc@5</th>\n",
       "      <th>Acc@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7153</td>\n",
       "      <td>0.8394</td>\n",
       "      <td>0.8467</td>\n",
       "      <td>0.3723</td>\n",
       "      <td>0.4453</td>\n",
       "      <td>0.2956</td>\n",
       "      <td>0.3431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file                  module         function        \n",
       "    Acc@1   Acc@3   Acc@5   Acc@5  Acc@10    Acc@5  Acc@10\n",
       "0  0.7153  0.8394  0.8467  0.3723  0.4453   0.2956  0.3431"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval with dataset\n",
    "locagent_loc_file = '../results/GPT4-1/loc_outputs.jsonl'\n",
    "locagent_res = evaluate_results(locagent_loc_file,\n",
    "                        level2key_dict,\n",
    "                        dataset='czlll/SWE-bench_Lite', split='test', \n",
    "                        metrics=['acc'],\n",
    "                        # metrics=['ndcg'],\n",
    "                        )\n",
    "locagent_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique folders in BM25 index: 300\n",
      "Sample BM25 folders: ['scikit-learn__scikit-learn-13779', 'django__django-16595', 'django__django-12113', 'django__django-14411', 'scikit-learn__scikit-learn-25500']\n",
      "Number of unique files in graph index: 300\n",
      "Number of unique instance IDs in loc_outputs.jsonl: 275\n",
      "Sample instance IDs: ['scikit-learn__scikit-learn-13779', 'django__django-16595', 'django__django-12113', 'django__django-14411', 'scikit-learn__scikit-learn-25500']\n",
      "\n",
      "=== Summary ===\n",
      "BM25 index files: 300\n",
      "Graph index files: 300\n",
      "Unique instance IDs in loc_outputs: 275\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up paths\n",
    "bm25_index_path = Path(\"../index_data/SWE-bench_Lite_bm25_13K/BM25_index\")\n",
    "graph_index_path = Path(\"../index_data/SWE-bench_Lite_bm25_13K/graph_index_v2.3\")\n",
    "loc_outputs_path = Path(\"../results/GPT4-1/loc_outputs.jsonl\")\n",
    "\n",
    "# Count unique folders (directories) in BM25 index\n",
    "bm25_folders = set()\n",
    "if bm25_index_path.exists():\n",
    "    for item in bm25_index_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            bm25_folders.add(item.name)\n",
    "    print(f\"Number of unique folders in BM25 index: {len(bm25_folders)}\")\n",
    "    print(f\"Sample BM25 folders: {list(bm25_folders)[:5]}\")\n",
    "else:\n",
    "    print(f\"BM25 index path not found: {bm25_index_path}\")\n",
    "    # Try to find the correct path\n",
    "    parent_path = Path(\"..\")\n",
    "    print(f\"Contents of parent directory: {list(parent_path.iterdir())[:10]}\")\n",
    "\n",
    "# Count unique files in graph index\n",
    "graph_files = set()\n",
    "if graph_index_path.exists():\n",
    "    for item in graph_index_path.iterdir():\n",
    "        if item.is_file():\n",
    "            graph_files.add(item.name)\n",
    "        elif item.is_dir():\n",
    "            # Also check subdirectories\n",
    "            for subitem in item.rglob(\"*\"):\n",
    "                if subitem.is_file():\n",
    "                    graph_files.add(subitem.relative_to(graph_index_path))\n",
    "    print(f\"Number of unique files in graph index: {len(graph_files)}\")\n",
    "else:\n",
    "    print(f\"Graph index path not found: {graph_index_path}\")\n",
    "\n",
    "# Count unique instance IDs in loc_outputs.jsonl\n",
    "unique_instance_ids = set()\n",
    "if loc_outputs_path.exists():\n",
    "    with open(loc_outputs_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    if 'instance_id' in data:\n",
    "                        unique_instance_ids.add(data['instance_id'])\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parsing line: {e}\")\n",
    "    print(f\"Number of unique instance IDs in loc_outputs.jsonl: {len(unique_instance_ids)}\")\n",
    "    \n",
    "    # Show first few instance IDs as a sample\n",
    "    print(f\"Sample instance IDs: {list(unique_instance_ids)[:5]}\")\n",
    "else:\n",
    "    print(f\"loc_outputs.jsonl not found at: {loc_outputs_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"BM25 index files: {len(bm25_folders)}\")\n",
    "print(f\"Graph index files: {len(graph_files)}\")\n",
    "print(f\"Unique instance IDs in loc_outputs: {len(unique_instance_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "Dataset sizes:\n",
      "czlll/SWE-bench_Lite: 274 instances\n",
      "princeton-nlp/SWE-bench_Lite_bm25_13K: 300 instances\n",
      "\n",
      "=== Instance ID Overlap ===\n",
      "Common instance IDs: 274\n",
      "Jaccard similarity (IDs): 0.913\n",
      "\n",
      "=== Problem Statement Comparison (for common instances) ===\n",
      "Instances with matching problem statements: 273/274 (99.6%)\n",
      "Instances with different problem statements: 1\n",
      "\n",
      "First 5 instances with different problem statements:\n",
      "\n",
      "- mwaskom__seaborn-2848\n",
      "  czlll length: 965 chars\n",
      "  princeton length: 5820 chars\n",
      "  czlll snippet: pairplot fails with hue_order not containing all hue values in seaborn 0.11.1\n",
      "In seaborn < 0.11, one...\n",
      "  princeton snippet: PairGrid errors with `hue` assigned in `map`\n",
      "In seaborn version 0.9.0 I was able to use the followin...\n",
      "\n",
      "=== SUMMARY ===\n",
      "✗ Datasets differ in:\n",
      "  - Number of instances: 274 vs 300\n",
      "  - Problem statements (1 differ)\n",
      "\n",
      "{'total_instances_czlll': 274, 'total_instances_princeton': 300, 'common_instance_ids': 274, 'jaccard_similarity': 0.9133333333333333, 'matching_problem_statements': 273, 'mismatched_problem_statements': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "print(\"Loading datasets...\")\n",
    "dataset1 = load_dataset(\"czlll/SWE-bench_Lite\", split=\"test\")\n",
    "dataset2 = load_dataset(\"princeton-nlp/SWE-bench_Lite_bm25_13K\", split=\"test\")\n",
    "\n",
    "# Create dictionaries mapping instance_id to problem_statement\n",
    "problems1 = {item['instance_id']: item.get('problem_statement', '') for item in dataset1}\n",
    "problems2 = {item['instance_id']: item.get('problem_statement', '') for item in dataset2}\n",
    "\n",
    "# Get instance ID sets\n",
    "ids1 = set(problems1.keys())\n",
    "ids2 = set(problems2.keys())\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"czlll/SWE-bench_Lite: {len(ids1)} instances\")\n",
    "print(f\"princeton-nlp/SWE-bench_Lite_bm25_13K: {len(ids2)} instances\")\n",
    "\n",
    "# Calculate instance ID overlap\n",
    "id_overlap = ids1.intersection(ids2)\n",
    "print(f\"\\n=== Instance ID Overlap ===\")\n",
    "print(f\"Common instance IDs: {len(id_overlap)}\")\n",
    "print(f\"Jaccard similarity (IDs): {len(id_overlap) / len(ids1.union(ids2)):.3f}\")\n",
    "\n",
    "# Check problem statement matches for common instances\n",
    "matching_problems = 0\n",
    "mismatched_problems = []\n",
    "\n",
    "for instance_id in id_overlap:\n",
    "    if problems1[instance_id] == problems2[instance_id]:\n",
    "        matching_problems += 1\n",
    "    else:\n",
    "        mismatched_problems.append(instance_id)\n",
    "\n",
    "print(f\"\\n=== Problem Statement Comparison (for common instances) ===\")\n",
    "print(f\"Instances with matching problem statements: {matching_problems}/{len(id_overlap)} ({matching_problems/len(id_overlap)*100:.1f}%)\")\n",
    "print(f\"Instances with different problem statements: {len(mismatched_problems)}\")\n",
    "\n",
    "if mismatched_problems:\n",
    "    print(f\"\\nFirst 5 instances with different problem statements:\")\n",
    "    for instance_id in mismatched_problems[:5]:\n",
    "        print(f\"\\n- {instance_id}\")\n",
    "        print(f\"  czlll length: {len(problems1[instance_id])} chars\")\n",
    "        print(f\"  princeton length: {len(problems2[instance_id])} chars\")\n",
    "        \n",
    "        # Show a snippet of the difference\n",
    "        if problems1[instance_id] and problems2[instance_id]:\n",
    "            print(f\"  czlll snippet: {problems1[instance_id][:100]}...\")\n",
    "            print(f\"  princeton snippet: {problems2[instance_id][:100]}...\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "if len(id_overlap) == len(ids1) == len(ids2) and matching_problems == len(id_overlap):\n",
    "    print(\"✓ Both datasets have identical instance IDs and problem statements!\")\n",
    "else:\n",
    "    print(\"✗ Datasets differ in:\")\n",
    "    if len(ids1) != len(ids2):\n",
    "        print(f\"  - Number of instances: {len(ids1)} vs {len(ids2)}\")\n",
    "    if len(id_overlap) < min(len(ids1), len(ids2)):\n",
    "        print(f\"  - Instance IDs (only {len(id_overlap)} common)\")\n",
    "    if matching_problems < len(id_overlap):\n",
    "        print(f\"  - Problem statements ({len(mismatched_problems)} differ)\")\n",
    "\n",
    "# Create a simple comparison report\n",
    "report = {\n",
    "    'total_instances_czlll': len(ids1),\n",
    "    'total_instances_princeton': len(ids2),\n",
    "    'common_instance_ids': len(id_overlap),\n",
    "    'jaccard_similarity': len(id_overlap) / len(ids1.union(ids2)),\n",
    "    'matching_problem_statements': matching_problems,\n",
    "    'mismatched_problem_statements': len(mismatched_problems)\n",
    "}\n",
    "\n",
    "print(f\"\\n{report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
